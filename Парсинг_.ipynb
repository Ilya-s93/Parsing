{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "\n",
    "# !pip3 install -U selenium\n",
    "\n",
    "# !pip3 install requests bs4 colorama\n",
    "\n",
    "# !pip install js2py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd983bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import unittest\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import colorama\n",
    "\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from lxml import html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f4660",
   "metadata": {},
   "source": [
    "## Получаем url всех страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_product_pages(url: str, pages_count: int) -> List[str]:\n",
    "    \n",
    "        # Получает список url страниц\n",
    "        # url = базовый url страницы конкретного продукта (data['url'])\n",
    "        # pages_count = Общее количество страниц для продукта.\n",
    "        \n",
    "    return list(map(lambda x: url + '/?page=' f\"{x}\", range(1, pages_count + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2713b",
   "metadata": {},
   "source": [
    "## Получаем все url-ссылки для конкретной страницы. Ссылки на авто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3538264",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorama.init()\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "YELLOW = colorama.Fore.YELLOW\n",
    "\n",
    "\n",
    "# ссылки url с переходом на url того же сайта\n",
    "internal_urls = set()\n",
    "# ссылки url с переходом на url другого сайта\n",
    "external_urls = set()\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Проверяет, является ли 'url' действительным URL\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_website_links(url: str, verify: bool = False, **kwargs) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Возвращает все URL-адреса, найденные на `url`, в котором он принадлежит тому же веб-сайту.\n",
    "    \"\"\"\n",
    "    # все URL-адреса `url`\n",
    "    urls = set()\n",
    "    # доменное имя URL без протокола\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url, verify=verify, **kwargs).content, 'html.parser')\n",
    "    \n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href пустой тег\n",
    "            continue\n",
    "        # присоединяемся к URL, если он относительный (не абсолютная ссылка)\n",
    "        href = urljoin(url, href)\n",
    "    \n",
    "        parsed_href = urlparse(href)\n",
    "        # удалить GET-параметры URL, фрагменты URL и т. д.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # недействительный URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # уже в наборе\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # внешняя ссылка\n",
    "            if href not in external_urls:\n",
    "                print(f\"{GRAY}[!] Внешняя ссылка: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        print(f\"{GREEN}[*] Внутренняя ссылка: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_all_website_links('https://...')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc454e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(a)\n",
    "spisok = list(filter(lambda x: 'sale' in x, a))\n",
    "spisok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99f56a",
   "metadata": {},
   "source": [
    "## Вытаскиваем html - код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_html(url: str, verify: bool = False, **kwargs) -> BeautifulSoup:\n",
    "    # Получает всю html страницы\n",
    "\n",
    "    time.sleep(10)\n",
    "    return BeautifulSoup(requests.get(url, verify=verify, **kwargs).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_data(soup: BeautifulSoup, url: str) -> Dict[str, str]:\n",
    "    '''Получает BeautifulSoup объект. Получает имя товара, и таблицу с характеристиками'''\n",
    "    \n",
    "    prod = soup.find('div', class_= ).find(\"a\", class_= ).text\n",
    "    prod = re.split(' ', prod)\n",
    "    prod = re.sub(r'[^\\d]','', prod)\n",
    "     \n",
    "    data_pars = datetime.now().date()\n",
    "    \n",
    "    _ = dict()\n",
    "    _.update({'url': i, 'ID': num_id, ..})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    time.sleep(5)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114cc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_amount_page(url: str, headers: Dict) -> int:\n",
    "\n",
    "    # Возвращает общее количество страниц для конкретного типа продукта\n",
    "    # url = базовый url страницы конкретного продукта (data['url'])\n",
    "    # headers = Шапка html запроса в формате словаря.\n",
    "\n",
    "\n",
    "    def page_count(tree):\n",
    "        return tree.xpath('//div[@class=\"ib page-num\"]//a[last()]/text()')\n",
    "\n",
    "    return int(page_count(html.fromstring(requests.get(url, headers=headers).content))[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
